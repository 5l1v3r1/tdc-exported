
# Jetson Xavier NX soll kleinster KI-Supercomputer sein

Published at: **2019-11-07T13:11:01+00:00**

Author: **Sebastian Grüner**

Original: [Golem.de](https://www.golem.de/news/nvidia-jetson-xavier-nx-soll-kleinster-ki-supercomputer-sein-1911-144856.html)

Hardwarehersteller Nvidia hat den Jetson Xavier NX vorgestellt, den das Unternehmen selbst als "kleinsten KI-Supercomputer" bezeichnet. Denn das als Steckmodul ausgeführte Board ist mit 70 x 45 mm sehr kompakt und bietet bei einer Leistungsaufnahme von nur 15 Watt bis zu 21 TOPS (INT8), was der Hersteller als Leistungsklasse für Server beschreibt. Bei einer Leistungsaufnahme von 10 Watt und entsprechend weniger Takt soll der Jetson Xavier NX immer noch 14 TOPS erreichen.
Der Jetson Xavier NX ist dabei kompatibel zu dem Jetson Nano, so dass Entwickler diesen als Hardware-Upgrade verwenden können. Wie der Name andeutet, bildet die Grundlage des Chips eine abgespeckte Variante der bisher auch schon als Xavier-Generation vertriebenen Tegra-SoCs.
Der Jetson Xavier NX nutzt sechs selbst entwickelte Kerne mit ARMs 64-Bit-Technik ARMv8.2. Diese heißen Carmel. Die CPU wird dabei in einer Big-Little-Architektur aus einem Cluster von zwei schnelleren und vier etwas langsameren Kernen gebildet. Hinzu kommen 384 Grafikkerne mit Volta-Technik samt 48-Tensor-Kernen und zwei speziellen Deep-Learning-Beschleunigern. Der Takt der CPU-Kerne sowie der GPU hängt von dem gewählten Energiebudget ab, also ob das Geräte mit 10 oder 15 Watt betrieben werden soll. Dem Minirechner stehen zudem 8 GByte LPDDR4 RAM und 16 GByte eMMC-Festspeicher zur Verfügung.
Der Jetson Xavier NX kann per 260-Pin-SO-DIMM an entsprechende Host-Platinen angeschlossen werden und soll ab März 2020 für rund 400 US-Dollar weltweit verfügbar sein. Das ist nicht nur deutlich teuer als etwa der Jetson Nano, sondern auch teurer als die Konkurrenz ähnlicher Entwicklerplatinen, etwa Googles Edge-TPUs oder die von Asus vorgestellten Tinker Boards mit KI-Beschleunigern. Jetson Xavier NX liefert dafür aber auch deutlich mehr Leistung für Deep-Learning-Aufgaben.
