
# Deep Fusion differences sometimes subtle, but really show up in pet photos

Published at: **2019-11-01T13:31:14+00:00**

Author: **Ben Lovejoy**

Original: [9to5Mac](https://9to5mac.com/2019/11/01/deep-fusion/)

iOS 13.2 landed earlier this week, bringing with it Deep Fusion, the feature Apple’s marketing chief described in the iPhone 11 keynote as ‘computational photography mad science.’ (Less happily, it also seemed to introduce problems with background apps.)
Like Smart HDR, Deep Fusion blends multiple frames to create the best possible composite image. But while Smart HDR is limited to blending different exposures, some designed to capture shadow areas, others to pull in highlights, Deep Fusion also aims to bring out more detail…
Here’s how Schiller described the feature, which is available only on the three iPhone 11 models.
Gizmodo’s Adam Estes has been testing the beta version for several weeks. One of the problems he faced that there’s no user indication of when Deep Fusion is active (though there are situations in which it definitely isn’t). You just have to take photos on two identical phones, one with iOS 13.2, one without, and look for differences.
The bad news is that, in many shots, the differences are only visible when pixel-peeping. They are not likely to make a noticeable difference to most people’s perception of a photo, he suggests.
Sometimes, though, the difference is immediately obvious. Although Estes describes as ‘subtle’ the difference between two shots of the clock in the middle of New York’s Grand Central Terminal, to me the difference is very clear even before viewing it full size.
Whether the Deep Fusion shot is better is a subjective judgment. It is certainly sharper, but perhaps borders on over-sharpened. Apps like Photoshop and Lightroom allow you to sharpen photos in post-production, and sometimes the result can look artificial if someone goes too far – and I would say that this is dangerously close to this territory. To the average person, however, sharper is ‘better,’ so Apple probably made the right call there.
Estes says there’s one case where the difference is night and day: photos of his puppy, Peanut, seen here in a close-up view.
What we’re seeing today is, of course, only the first iteration of Deep Fusion, and Estes speculates that future updates will improve the zoom capabilities, much as Google has with the Pixel 4.
Are you noticing significant differences in photos before and after updating to iOS 13.2? Please let us know in the comments.
Check out the full piece here.
FTC: We use income earning auto affiliate links. More.
